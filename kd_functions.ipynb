{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da90f6c4-1d02-4d82-8787-04b4faae6ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/HiLab-git/SSL4MIS/blob/master/code/train_mean_teacher_2D.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bedda228-2aba-40aa-b59a-e13724454751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.nn.modules.loss import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92a89fc2-dcce-47f2-b17d-8e048f5b1cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_factory(net_type=\"unet\", in_chns=1, class_num=3):\n",
    "    if net_type == \"unet\":\n",
    "        net = UNet(in_chns=in_chns, class_num=class_num).cuda()\n",
    "    elif net_type == \"enet\":\n",
    "        net = ENet(in_channels=in_chns, num_classes=class_num).cuda()\n",
    "    elif net_type == \"unet_ds\":\n",
    "        net = UNet_DS(in_chns=in_chns, class_num=class_num).cuda()\n",
    "    elif net_type == \"unet_cct\":\n",
    "        net = UNet_CCT(in_chns=in_chns, class_num=class_num).cuda()\n",
    "    elif net_type == \"unet_urpc\":\n",
    "        net = UNet_URPC(in_chns=in_chns, class_num=class_num).cuda()\n",
    "    elif net_type == \"efficient_unet\":\n",
    "        net = Effi_UNet('efficientnet-b3', encoder_weights='imagenet',\n",
    "                        in_channels=in_chns, classes=class_num).cuda()\n",
    "    elif net_type == \"ViT_Seg\":\n",
    "        net = ViT_seg(config, img_size=args.patch_size,\n",
    "                      num_classes=args.num_classes).cuda()\n",
    "    elif net_type == \"pnet\":\n",
    "        net = PNet2D(in_chns, class_num, 64, [1, 2, 4, 8, 16]).cuda()\n",
    "    elif net_type == \"nnUNet\":\n",
    "        net = initialize_network(num_classes=class_num).cuda()\n",
    "    else:\n",
    "        net = None\n",
    "    return net\n",
    "\n",
    "\n",
    "\n",
    "def dice_loss(score, target):\n",
    "    target = target.float()\n",
    "    smooth = 1e-5\n",
    "    intersect = torch.sum(score * target)\n",
    "    y_sum = torch.sum(target * target)\n",
    "    z_sum = torch.sum(score * score)\n",
    "    loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)\n",
    "    loss = 1 - loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "def dice_loss1(score, target):\n",
    "    target = target.float()\n",
    "    smooth = 1e-5\n",
    "    intersect = torch.sum(score * target)\n",
    "    y_sum = torch.sum(target)\n",
    "    z_sum = torch.sum(score)\n",
    "    loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)\n",
    "    loss = 1 - loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "def entropy_loss(p, C=2):\n",
    "    # p N*C*W*H*D\n",
    "    y1 = -1*torch.sum(p*torch.log(p+1e-6), dim=1) / \\\n",
    "        torch.tensor(np.log(C)).cuda()\n",
    "    ent = torch.mean(y1)\n",
    "\n",
    "    return ent\n",
    "\n",
    "\n",
    "def softmax_dice_loss(input_logits, target_logits):\n",
    "    \"\"\"Takes softmax on both sides and returns MSE loss\n",
    "\n",
    "    Note:\n",
    "    - Returns the sum over all examples. Divide by the batch size afterwards\n",
    "      if you want the mean.\n",
    "    - Sends gradients to inputs but not the targets.\n",
    "    \"\"\"\n",
    "    assert input_logits.size() == target_logits.size()\n",
    "    input_softmax = F.softmax(input_logits, dim=1)\n",
    "    target_softmax = F.softmax(target_logits, dim=1)\n",
    "    n = input_logits.shape[1]\n",
    "    dice = 0\n",
    "    for i in range(0, n):\n",
    "        dice += dice_loss1(input_softmax[:, i], target_softmax[:, i])\n",
    "    mean_dice = dice / n\n",
    "\n",
    "    return mean_dice\n",
    "\n",
    "\n",
    "def entropy_loss_map(p, C=2):\n",
    "    ent = -1*torch.sum(p * torch.log(p + 1e-6), dim=1,\n",
    "                       keepdim=True)/torch.tensor(np.log(C)).cuda()\n",
    "    return ent\n",
    "\n",
    "\n",
    "def softmax_mse_loss(input_logits, target_logits, sigmoid=False):\n",
    "    \"\"\"Takes softmax on both sides and returns MSE loss\n",
    "\n",
    "    Note:\n",
    "    - Returns the sum over all examples. Divide by the batch size afterwards\n",
    "      if you want the mean.\n",
    "    - Sends gradients to inputs but not the targets.\n",
    "    \"\"\"\n",
    "    assert input_logits.size() == target_logits.size()\n",
    "    if sigmoid:\n",
    "        input_softmax = torch.sigmoid(input_logits)\n",
    "        target_softmax = torch.sigmoid(target_logits)\n",
    "    else:\n",
    "        input_softmax = F.softmax(input_logits, dim=1)\n",
    "        target_softmax = F.softmax(target_logits, dim=1)\n",
    "\n",
    "    mse_loss = (input_softmax-target_softmax)**2\n",
    "    return mse_loss\n",
    "\n",
    "\n",
    "def softmax_kl_loss(input_logits, target_logits, sigmoid=False):\n",
    "    \"\"\"Takes softmax on both sides and returns KL divergence\n",
    "\n",
    "    Note:\n",
    "    - Returns the sum over all examples. Divide by the batch size afterwards\n",
    "      if you want the mean.\n",
    "    - Sends gradients to inputs but not the targets.\n",
    "    \"\"\"\n",
    "    assert input_logits.size() == target_logits.size()\n",
    "    if sigmoid:\n",
    "        input_log_softmax = torch.log(torch.sigmoid(input_logits))\n",
    "        target_softmax = torch.sigmoid(target_logits)\n",
    "    else:\n",
    "        input_log_softmax = F.log_softmax(input_logits, dim=1)\n",
    "        target_softmax = F.softmax(target_logits, dim=1)\n",
    "\n",
    "    # return F.kl_div(input_log_softmax, target_softmax)\n",
    "    kl_div = F.kl_div(input_log_softmax, target_softmax, reduction='mean')\n",
    "    # mean_kl_div = torch.mean(0.2*kl_div[:,0,...]+0.8*kl_div[:,1,...])\n",
    "    return kl_div\n",
    "\n",
    "\n",
    "def symmetric_mse_loss(input1, input2):\n",
    "    \"\"\"Like F.mse_loss but sends gradients to both directions\n",
    "\n",
    "    Note:\n",
    "    - Returns the sum over all examples. Divide by the batch size afterwards\n",
    "      if you want the mean.\n",
    "    - Sends gradients to both input1 and input2.\n",
    "    \"\"\"\n",
    "    assert input1.size() == input2.size()\n",
    "    return torch.mean((input1 - input2)**2)\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha, (float, int)):\n",
    "            self.alpha = torch.Tensor([alpha, 1-alpha])\n",
    "        if isinstance(alpha, list):\n",
    "            self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim() > 2:\n",
    "            # N,C,H,W => N,C,H*W\n",
    "            input = input.view(input.size(0), input.size(1), -1)\n",
    "            input = input.transpose(1, 2)    # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1, input.size(2))   # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1, 1)\n",
    "\n",
    "        logpt = F.log_softmax(input, dim=1)\n",
    "        logpt = logpt.gather(1, target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type() != input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0, target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss.sum()\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def _one_hot_encoder(self, input_tensor):\n",
    "        tensor_list = []\n",
    "        for i in range(self.n_classes):\n",
    "            temp_prob = input_tensor == i * torch.ones_like(input_tensor)\n",
    "            tensor_list.append(temp_prob)\n",
    "        output_tensor = torch.cat(tensor_list, dim=1)\n",
    "        return output_tensor.float()\n",
    "\n",
    "    def _dice_loss(self, score, target):\n",
    "        target = target.float()\n",
    "        smooth = 1e-5\n",
    "        intersect = torch.sum(score * target)\n",
    "        y_sum = torch.sum(target * target)\n",
    "        z_sum = torch.sum(score * score)\n",
    "        loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)\n",
    "        loss = 1 - loss\n",
    "        return loss\n",
    "\n",
    "    def forward(self, inputs, target, weight=None, softmax=False):\n",
    "        if softmax:\n",
    "            inputs = torch.softmax(inputs, dim=1)\n",
    "        target = self._one_hot_encoder(target)\n",
    "        if weight is None:\n",
    "            weight = [1] * self.n_classes\n",
    "        assert inputs.size() == target.size(), 'predict & target shape do not match'\n",
    "        class_wise_dice = []\n",
    "        loss = 0.0\n",
    "        for i in range(0, self.n_classes):\n",
    "            dice = self._dice_loss(inputs[:, i], target[:, i])\n",
    "            class_wise_dice.append(1.0 - dice.item())\n",
    "            loss += dice * weight[i]\n",
    "        return loss / self.n_classes\n",
    "\n",
    "\n",
    "def entropy_minmization(p):\n",
    "    y1 = -1*torch.sum(p*torch.log(p+1e-6), dim=1)\n",
    "    ent = torch.mean(y1)\n",
    "\n",
    "    return ent\n",
    "\n",
    "\n",
    "def entropy_map(p):\n",
    "    ent_map = -1*torch.sum(p * torch.log(p + 1e-6), dim=1,\n",
    "                           keepdim=True)\n",
    "    return ent_map\n",
    "\n",
    "\n",
    "def compute_kl_loss(p, q):\n",
    "    p_loss = F.kl_div(F.log_softmax(p, dim=-1),\n",
    "                      F.softmax(q, dim=-1), reduction='none')\n",
    "    q_loss = F.kl_div(F.log_softmax(q, dim=-1),\n",
    "                      F.softmax(p, dim=-1), reduction='none')\n",
    "\n",
    "    # Using function \"sum\" and \"mean\" are depending on your task\n",
    "    p_loss = p_loss.mean()\n",
    "    q_loss = q_loss.mean()\n",
    "\n",
    "    loss = (p_loss + q_loss) / 2\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71b47b51-18ec-4767-a84a-5d485656fc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from medpy import metric\n",
    "\n",
    "\n",
    "def cal_dice(prediction, label, num=2):\n",
    "    total_dice = np.zeros(num-1)\n",
    "    for i in range(1, num):\n",
    "        prediction_tmp = (prediction == i)\n",
    "        label_tmp = (label == i)\n",
    "        prediction_tmp = prediction_tmp.astype(np.float)\n",
    "        label_tmp = label_tmp.astype(np.float)\n",
    "\n",
    "        dice = 2 * np.sum(prediction_tmp * label_tmp) / (np.sum(prediction_tmp) + np.sum(label_tmp))\n",
    "        total_dice[i - 1] += dice\n",
    "\n",
    "    return total_dice\n",
    "\n",
    "\n",
    "def calculate_metric_percase(pred, gt):\n",
    "    dc = metric.binary.dc(pred, gt)\n",
    "    jc = metric.binary.jc(pred, gt)\n",
    "    hd = metric.binary.hd95(pred, gt)\n",
    "    asd = metric.binary.asd(pred, gt)\n",
    "\n",
    "    return dc, jc, hd, asd\n",
    "\n",
    "\n",
    "def dice(input, target, ignore_index=None):\n",
    "    smooth = 1.\n",
    "    # using clone, so that it can do change to original target.\n",
    "    iflat = input.clone().view(-1)\n",
    "    tflat = target.clone().view(-1)\n",
    "    if ignore_index is not None:\n",
    "        mask = tflat == ignore_index\n",
    "        tflat[mask] = 0\n",
    "        iflat[mask] = 0\n",
    "    intersection = (iflat * tflat).sum()\n",
    "\n",
    "    return (2. * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth)\n",
    "\n",
    "\n",
    "def sigmoid_rampup(current, rampup_length):\n",
    "    \"\"\"Exponential rampup from https://arxiv.org/abs/1610.02242\"\"\"\n",
    "    if rampup_length == 0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        current = np.clip(current, 0.0, rampup_length)\n",
    "        phase = 1.0 - current / rampup_length\n",
    "        return float(np.exp(-5.0 * phase * phase))\n",
    "\n",
    "\n",
    "def linear_rampup(current, rampup_length):\n",
    "    \"\"\"Linear rampup\"\"\"\n",
    "    assert current >= 0 and rampup_length >= 0\n",
    "    if current >= rampup_length:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return current / rampup_length\n",
    "\n",
    "\n",
    "def cosine_rampdown(current, rampdown_length):\n",
    "    \"\"\"Cosine rampdown from https://arxiv.org/abs/1608.03983\"\"\"\n",
    "    assert 0 <= current <= rampdown_length\n",
    "    return float(.5 * (np.cos(np.pi * current / rampdown_length) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e12bde1-d2a7-439d-a12c-7c7f02f16e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import zoom\n",
    "\n",
    "\n",
    "def calculate_metric_percase(pred, gt):\n",
    "    pred[pred > 0] = 1\n",
    "    gt[gt > 0] = 1\n",
    "    if pred.sum() > 0:\n",
    "        dice = metric.binary.dc(pred, gt)\n",
    "        hd95 = metric.binary.hd95(pred, gt)\n",
    "        return dice, hd95\n",
    "    else:\n",
    "        return 0, 0\n",
    "\n",
    "\n",
    "def test_single_volume(image, label, net, classes, patch_size=[256, 256]):\n",
    "    image, label = image.squeeze(0).cpu().detach(\n",
    "    ).numpy(), label.squeeze(0).cpu().detach().numpy()\n",
    "    prediction = np.zeros_like(label)\n",
    "    for ind in range(image.shape[0]):\n",
    "        slice = image[ind, :, :]\n",
    "        x, y = slice.shape[0], slice.shape[1]\n",
    "        slice = zoom(slice, (patch_size[0] / x, patch_size[1] / y), order=0)\n",
    "        input = torch.from_numpy(slice).unsqueeze(\n",
    "            0).unsqueeze(0).float().cuda()\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            out = torch.argmax(torch.softmax(\n",
    "                net(input), dim=1), dim=1).squeeze(0)\n",
    "            out = out.cpu().detach().numpy()\n",
    "            pred = zoom(out, (x / patch_size[0], y / patch_size[1]), order=0)\n",
    "            prediction[ind] = pred\n",
    "    metric_list = []\n",
    "    for i in range(1, classes):\n",
    "        metric_list.append(calculate_metric_percase(\n",
    "            prediction == i, label == i))\n",
    "    return metric_list\n",
    "\n",
    "\n",
    "def test_single_volume_ds(image, label, net, classes, patch_size=[256, 256]):\n",
    "    image, label = image.squeeze(0).cpu().detach(\n",
    "    ).numpy(), label.squeeze(0).cpu().detach().numpy()\n",
    "    prediction = np.zeros_like(label)\n",
    "    for ind in range(image.shape[0]):\n",
    "        slice = image[ind, :, :]\n",
    "        x, y = slice.shape[0], slice.shape[1]\n",
    "        slice = zoom(slice, (patch_size[0] / x, patch_size[1] / y), order=0)\n",
    "        input = torch.from_numpy(slice).unsqueeze(\n",
    "            0).unsqueeze(0).float().cuda()\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            output_main, _, _, _ = net(input)\n",
    "            out = torch.argmax(torch.softmax(\n",
    "                output_main, dim=1), dim=1).squeeze(0)\n",
    "            out = out.cpu().detach().numpy()\n",
    "            pred = zoom(out, (x / patch_size[0], y / patch_size[1]), order=0)\n",
    "            prediction[ind] = pred\n",
    "    metric_list = []\n",
    "    for i in range(1, classes):\n",
    "        metric_list.append(calculate_metric_percase(\n",
    "            prediction == i, label == i))\n",
    "    return metric_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cac198f4-b1e7-47a2-bd40-48cc20880b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--root_path', type=str,\n",
    "#                     default='../data/ACDC', help='Name of Experiment')\n",
    "# parser.add_argument('--exp', type=str,\n",
    "#                     default='ACDC/Mean_Teacher', help='experiment_name')\n",
    "# parser.add_argument('--model', type=str,\n",
    "#                     default='unet', help='model_name')\n",
    "# parser.add_argument('--max_iterations', type=int,\n",
    "#                     default=30000, help='maximum epoch number to train')\n",
    "# parser.add_argument('--batch_size', type=int, default=24,\n",
    "#                     help='batch_size per gpu')\n",
    "# parser.add_argument('--deterministic', type=int,  default=1,\n",
    "#                     help='whether use deterministic training')\n",
    "# parser.add_argument('--base_lr', type=float,  default=0.01,\n",
    "#                     help='segmentation network learning rate')\n",
    "# parser.add_argument('--patch_size', type=list,  default=[256, 256],\n",
    "#                     help='patch size of network input')\n",
    "# parser.add_argument('--seed', type=int,  default=1337, help='random seed')\n",
    "# parser.add_argument('--num_classes', type=int,  default=4,\n",
    "#                     help='output channel of network')\n",
    "\n",
    "# # label and unlabel\n",
    "# parser.add_argument('--labeled_bs', type=int, default=12,\n",
    "#                     help='labeled_batch_size per gpu')\n",
    "# parser.add_argument('--labeled_num', type=int, default=136,\n",
    "#                     help='labeled data')\n",
    "# # costs\n",
    "# parser.add_argument('--ema_decay', type=float,  default=0.99, help='ema_decay')\n",
    "# parser.add_argument('--consistency_type', type=str,\n",
    "#                     default=\"mse\", help='consistency_type')\n",
    "# parser.add_argument('--consistency', type=float,\n",
    "#                     default=0.1, help='consistency')\n",
    "# parser.add_argument('--consistency_rampup', type=float,\n",
    "#                     default=200.0, help='consistency_rampup')\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95cc911a-2dc8-4c27-8c07-0f9c490885c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4f64cfd-47b2-44a7-8ea9-8e293b05ee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def patients_to_slices(dataset, patiens_num):\n",
    "    ref_dict = None\n",
    "    if \"ACDC\" in dataset:\n",
    "        ref_dict = {\"3\": 68, \"7\": 136,\n",
    "                    \"14\": 256, \"21\": 396, \"28\": 512, \"35\": 664, \"140\": 1312}\n",
    "    elif \"Prostate\":\n",
    "        ref_dict = {\"2\": 27, \"4\": 53, \"8\": 120,\n",
    "                    \"12\": 179, \"16\": 256, \"21\": 312, \"42\": 623}\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "    return ref_dict[str(patiens_num)]\n",
    "\n",
    "\n",
    "def get_current_consistency_weight(epoch):\n",
    "    # Consistency ramp-up from https://arxiv.org/abs/1610.02242\n",
    "    return args.consistency * ramps.sigmoid_rampup(epoch, args.consistency_rampup)\n",
    "\n",
    "\n",
    "def update_ema_variables(model, ema_model, alpha, global_step):\n",
    "    # Use the true average until the exponential average is more correct\n",
    "    alpha = min(1 - 1 / (global_step + 1), alpha)\n",
    "    for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
    "        ema_param.data.mul_(alpha).add_(1 - alpha, param.data)\n",
    "\n",
    "\n",
    "def train(args, snapshot_path):\n",
    "    base_lr = args.base_lr\n",
    "    num_classes = args.num_classes\n",
    "    batch_size = args.batch_size\n",
    "    max_iterations = args.max_iterations\n",
    "\n",
    "    def create_model(ema=False):\n",
    "        # Network definition\n",
    "        model = net_factory(net_type=args.model, in_chns=1,\n",
    "                            class_num=num_classes)\n",
    "        if ema:\n",
    "            for param in model.parameters():\n",
    "                param.detach_()\n",
    "        return model\n",
    "\n",
    "    model = create_model()\n",
    "    ema_model = create_model(ema=True)\n",
    "\n",
    "    def worker_init_fn(worker_id):\n",
    "        random.seed(args.seed + worker_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c79dc3c1-e9be-4c27-a532-5ab0cd933a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_train = BaseDataSets(base_dir=args.root_path, split=\"train\", num=None, transform=transforms.Compose([\n",
    "#         RandomGenerator(args.patch_size)\n",
    "#     ]))\n",
    "# db_val = BaseDataSets(base_dir=args.root_path, split=\"val\")\n",
    "\n",
    "# total_slices = len(db_train)\n",
    "# labeled_slice = patients_to_slices(args.root_path, args.labeled_num)\n",
    "# print(\"Total silices is: {}, labeled slices is: {}\".format(\n",
    "#     total_slices, labeled_slice))\n",
    "# labeled_idxs = list(range(0, labeled_slice))\n",
    "# unlabeled_idxs = list(range(labeled_slice, total_slices))\n",
    "# batch_sampler = TwoStreamBatchSampler(\n",
    "#     labeled_idxs, unlabeled_idxs, batch_size, batch_size-args.labeled_bs)\n",
    "\n",
    "# trainloader = DataLoader(db_train, batch_sampler=batch_sampler,\n",
    "#                          num_workers=4, pin_memory=True, worker_init_fn=worker_init_fn)\n",
    "\n",
    "# model.train()\n",
    "\n",
    "# valloader = DataLoader(db_val, batch_size=1, shuffle=False,\n",
    "#                        num_workers=1)\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=base_lr,\n",
    "#                       momentum=0.9, weight_decay=0.0001)\n",
    "# ce_loss = CrossEntropyLoss()\n",
    "# dice_loss = losses.DiceLoss(num_classes)\n",
    "\n",
    "# writer = SummaryWriter(snapshot_path + '/log')\n",
    "# logging.info(\"{} iterations per epoch\".format(len(trainloader)))\n",
    "\n",
    "# iter_num = 0\n",
    "# max_epoch = max_iterations // len(trainloader) + 1\n",
    "# best_performance = 0.0\n",
    "# iterator = tqdm(range(max_epoch), ncols=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fbfc3b6-6690-4775-97ac-8562ada9fc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_current_consistency_weight(epoch):\n",
    "#     # Consistency ramp-up from https://arxiv.org/abs/1610.02242\n",
    "#     return args.consistency * ramps.sigmoid_rampup(epoch, args.consistency_rampup)\n",
    "\n",
    "\n",
    "def update_ema_variables(model, ema_model, alpha, global_step):\n",
    "    # Use the true average until the exponential average is more correct\n",
    "    alpha = min(1 - 1 / (global_step + 1), alpha)\n",
    "    for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
    "        ema_param.data.mul_(alpha).add_(1 - alpha, param.data)\n",
    "\n",
    "\n",
    "# def train(args, snapshot_path):\n",
    "#     base_lr = args.base_lr\n",
    "#     num_classes = args.num_classes\n",
    "#     batch_size = args.batch_size\n",
    "#     max_iterations = args.max_iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11dc59f-cda9-4dfe-b6f1-6aad86d92a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
