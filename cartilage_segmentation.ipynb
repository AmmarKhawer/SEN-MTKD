{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "186e809a-7ced-4bb5-be1c-2ab90f6ac898",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run dataset.ipynb\n",
    "%run models.ipynb\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from operator import add\n",
    "from PIL import Image\n",
    "import segmentation_models_pytorch as smp\n",
    "import os \n",
    "import os\n",
    "# os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "# import tensorflow as tf \n",
    "import re\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score, recall_score\n",
    "import import_ipynb\n",
    "# import segmentation_models as sm\n",
    "\n",
    "import json\n",
    "from pyfiles.efficientunet import *\n",
    "\n",
    "#!pip install pretrained-backbones-unet\n",
    "\n",
    "from backbones_unet.model.unet import Unet\n",
    "from backbones_unet.utils.dataset import SemanticSegmentationDataset\n",
    "from backbones_unet.utils.trainer import Trainer\n",
    "import backbones_unet\n",
    "import os \n",
    "# )\n",
    "\n",
    "# params = [p for p in model.parameters() if p.requires_grad]\n",
    "# optimizer = torch.optim.AdamW(params, 1e-4) \n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model,                    # UNet model with pretrained backbone\n",
    "#     criterion=DiceLoss(),     # loss function for model convergence\n",
    "#     optimizer=optimizer,      # optimizer for regularization\n",
    "#     epochs=10                 # number of epochs for model training\n",
    "# )\n",
    "\n",
    "# trainer.fit(train_loader, val_loader)\n",
    "# print(backbones_unet.__available_models__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b8045d6-7ec5-4e5c-94a3-e9d5662f2545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #loading the json file\n",
    "# import json\n",
    "# # Load the JSON file\n",
    "# with open('dict_indices.json', 'r') as file:\n",
    "#     data = json.load(file)\n",
    "# train_indices = data['train_indices']\n",
    "# val_indices = data['val_indices']\n",
    "\n",
    "# # Load the JSON file\n",
    "# with open('domainb_indices.json', 'r') as file:\n",
    "#     data = json.load(file)\n",
    "\n",
    "# domainb_train = data['train']\n",
    "# domainb_val = data['val']\n",
    "\n",
    "import json\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import re\n",
    "\n",
    "\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "\n",
    "# Load the JSON file\n",
    "with open('dict_indices.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "train_indices = data['train_indices']\n",
    "val_indices = data['val_indices']\n",
    "\n",
    "with open('domainb_indices.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "domainb_indices = data['train']\n",
    "\n",
    "\n",
    "train_path = os.path.join(os.getcwd(),\"train\")\n",
    "mask_path = os.path.join(os.getcwd(),\"train_gt\")\n",
    "train_x = os.listdir('train')\n",
    "train_x.sort(key=extract_numerical_part)\n",
    "train_y = os.listdir('train_gt')\n",
    "train_y.sort(key=extract_numerical_part)\n",
    "\n",
    "val_path = os.path.join(os.getcwd(),\"val\")\n",
    "val_gt = os.path.join(os.getcwd(),\"val_gt\")\n",
    "val_x = os.listdir('val')\n",
    "val_x.sort(key=extract_numerical_part)\n",
    "val_y = os.listdir('val_gt')\n",
    "val_y.sort(key=extract_numerical_part)\n",
    "\n",
    "testB = os.path.join(os.getcwd(),\"imgs\")\n",
    "test_gtB = os.path.join(os.getcwd(),\"Gts\")\n",
    "path_soft_gtB  =  os.path.join(os.getcwd(),\"soft_gtB\")\n",
    "\n",
    "testB_images = os.listdir(testB)\n",
    "testB_images.sort(key=extract_numerical_part)\n",
    "testB_masks = os.listdir(test_gtB)\n",
    "testB_masks.sort(key=extract_numerical_part)\n",
    "\n",
    "\n",
    "\n",
    "vss = dess400(train_path,mask_path,train_indices,train_transform)\n",
    "vall = dess400(val_path,val_gt,val_indices,valid_transform)\n",
    "dbb = dess400(testB,test_gtB,domainb_indices,train_transform)\n",
    "# softb = dess400(testB,path_soft_gtB,domainb_train , train_transform)\n",
    "# testb_val = dess400(testB, test_gtB ,domainb_val,train_transform)\n",
    "\n",
    "# combined_ds_train = ConcatDataset([vss,softb ])\n",
    "\n",
    "\n",
    "# train_loader = DataLoader(vss, batch_size=5, shuffle=True)\n",
    "# val_loader = DataLoader(vall, batch_size=15, shuffle=False)\n",
    "\n",
    "# mix_loader = DataLoader(combined_ds_train, batch_size=10 , shuffle=True)\n",
    "\n",
    "vs= Dess2(train_path,mask_path,train_x ,train_y , train_transform)\n",
    "\n",
    "val= Dess2(val_path , val_gt, val_x,val_y , valid_transform)\n",
    "domainb = hotencoded(testB , test_gtB , testB_images    , testB_masks , num_classes=7,transform=valid_transform)\n",
    "domainb_soft = hot400(testB , path_soft_gtB, domainb_indices, num_classes=7,transform=train_transform)\n",
    "domainb_small = hot400(testB , test_gtB,domainb_indices,num_classes=7,transform=valid_transform)\n",
    "ds = hot400(train_path , mask_path,train_indices,num_classes=7,transform=train_transform)\n",
    "ds_val = hot400(val_path , val_gt , val_x,num_classes=7,transform=valid_transform)\n",
    "# ds = hotencoded(train_path,mask_path , train_x,train_y , num_classes=7,transform=train_transform)\n",
    "# ds_val = hotencoded(val_path,val_gt,val_x,val_y,num_classes=7,transform=valid_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e91509c3-9634-4265-8823-f591c68d8013",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in ds_val:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f6468d-3639-48ac-8a8b-588dccff5938",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_indices), len(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d861ecc7-55fe-4b3c-ace1-f332f1cf0102",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de671cd8-f0ed-4222-af5a-af7af3799008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def train(model, loader, optimizer, loss_fn, device):\n",
    "    epoch_loss = 0.0\n",
    "    count = 0\n",
    "    model.train()\n",
    "    for x, y,_ ,_ in loader:\n",
    "        x = x.to(device, dtype=torch.float32)\n",
    "        y = y.float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() \n",
    "        count = count + y.shape[0]\n",
    "        print(count , \" images processed\")\n",
    "\n",
    "    epoch_loss = epoch_loss/len(loader)\n",
    "    return epoch_loss\n",
    "\n",
    "def evaluate(model, loader, loss_fn, device):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y,_ ,_ in loader:\n",
    "            x = x.to(device, dtype=torch.float32)\n",
    "            y = y.float().to(device)\n",
    "\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        epoch_loss = epoch_loss / len(loader)\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ef3b33-8c8d-460c-94d8-7bd60534a7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Unet(\n",
    "#     backbone='efficientnet_b4', # backbone network name\n",
    "#     in_channels=3,            # input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "#     num_classes=7,            # output channels (number of classes in your dataset)\n",
    "# )\n",
    "# #1000 mb model size around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e043ffe-0986-495d-b54f-00ffde6820ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# model = smp.Unet(\n",
    "#     encoder_name=\"efficientnet-b4\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "#     encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "#     in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "#     classes=7,                      # model output channels (number of classes in your dataset)\n",
    "# )\n",
    "\n",
    "# model.load_state_dict(torch.load(\"buildunet-3.pth\" , map_location=torch.device(device)))\n",
    "# model.load_state_dict(torch.load(\"efb4-full.pth\" , map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879edf42-c7d1-454e-86d6-6a768309d0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# img,_,mask,_=ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a991b7be-2014-4b7e-a8af-22c9df4904e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24766448-eef1-4d8e-b18d-1e5e822a648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_efficientunet_b4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22365dd4-5c61-4f49-a314-e765deccf756",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_efficientunet_b4(out_channels=7, concat_input=True, pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e19194f-12f8-439c-96e8-558719c423d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=7,                      # model output channels (number of classes in your dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "837d871d-ae3f-4cf2-b37b-def7a313bd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_model = smp.Unet(\n",
    "    encoder_name=\"efficientnet-b4\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=7,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "# old_model.load_state_dict(torch.load(\"efb4-full.pth\",map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "389a2e13-0c94-45bb-bbbf-2262f2deede4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"newefb4svd-full.pth\"))\n",
    "\n",
    "# model.load_state_dict(torch.load(\"newefb4svd-3200.pth\" , map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a231352-e20d-4909-aaa5-4d4585182f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "checkpoint_path = \".\"\n",
    "# Set cuda device\n",
    "# model = build_unet()\n",
    "\n",
    "# model =build_unet()\n",
    "\n",
    "model = model.to(device)\n",
    "# model.load_state_dict(torch.load(\"checkpoint.pth\" , map_location=torch.device(device)))\n",
    "\n",
    "# train_loader = DataLoader(vss , batch_size = 8,shuffle=True , num_workers=4)\n",
    "\n",
    "# val_loader = DataLoader(vall , batch_size = 25,shuffle =False)\n",
    "train_loader = DataLoader(ds, batch_size = 8,shuffle=True , num_workers=4)\n",
    "val_loader = DataLoader(ds_val , batch_size = 20,shuffle =False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11ed951b-4bcf-4c29-a3e6-01b69655e1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Optimizer and Loss\n",
    "\n",
    "num_epochs = 100\n",
    "learning_rate =0.0001\n",
    "\n",
    "# # loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn = CategoricalFocalLoss(gamma=2.0, alpha=None, reduction='mean')\n",
    "\n",
    "# loss_fn = DiceLoss(n_classes=7,softmax=True,weight=[1,2,1,2,2,2,1])\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8102f55-8a4a-4ad8-b197-3ab8cabf8051",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Teacher\n",
    "train_losses=[]\n",
    "valid_losses = []\n",
    "best_valid_loss = float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a49ec7-2cab-435d-afd8-f32f1e40167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_loss=b_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e646a8-5e69-46fb-b6d5-4835951a6d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(domainb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae36376c-811c-42c5-bc58-1443b3f25880",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#9 done 1-4lr to 5-5 to 1-5\n",
    "#softmax 2 epochs increase f1 \n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_loader, optimizer, loss_fn, device)\n",
    "    # a_loss, a_f1, a_f1_class = validlossf1(model, ds_val, loss_fn, num_classes=7)\n",
    "    b_loss, b_f1, b_f1_class = validlossf1(model, ds_val, loss_fn, num_classes=7)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(b_loss)\n",
    "\n",
    "    # Saving the model\n",
    "    if b_loss < best_valid_loss:\n",
    "        print(f\"Valid loss improved from {best_valid_loss:2.4f} to {b_loss:2.4f}. Saving checkpoint: {checkpoint_path}\")\n",
    "\n",
    "        best_valid_loss = a_loss\n",
    "        torch.save(model.state_dict(), \"resnet34.pth\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    data_str = f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\\n'\n",
    "    data_str += f'\\tTrain Loss: {train_loss:.3f}\\n'\n",
    "    data_str += f'\\t. Loss: {b_f1:.3f}\\n'\n",
    "    print(data_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca005e3-7315-4c4b-8b4b-375ad7ffee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"newefb4svd-3200.pth\" , map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e14922d-e6a0-406f-81f4-150809a4a7c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_model.load_state_dict(torch.load(\"efb4-full.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "519cdf2b-d95d-4946-96e5-7b19e4919f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2090700/3008366362.py:25: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  precision = true_positives / (true_positives + false_positives)\n",
      "/tmp/ipykernel_2090700/3008366362.py:26: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  recall = true_positives / (true_positives + false_negatives)\n",
      "/tmp/ipykernel_2090700/3008366362.py:28: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  f1_scores[class_label] = 2 * (precision * recall) / (precision + recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.86851\n"
     ]
    }
   ],
   "source": [
    "old_model.to(device)\n",
    "i,t,tt=validlossf1(old_model,ds_val,loss_fn,num_classes=7)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286a29cd-b46c-4a83-91c4-5dbf537f40b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "i,t,tt=validlossf2(model,ds_val,loss_fn,num_classes=7)\n",
    "print(t)\n",
    "# i,t,tt=validlossf1(model2,ds_val,loss_fn,num_classes=7)\n",
    "# print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381533ec-95fb-4002-9b3c-b4902d0e76a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce6ff5a-1e27-4565-ab76-852be7e97941",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(domainb_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9464869b-c87b-465a-b985-d3a211f00f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"newefb4svd-full.pth\" , map_location=torch.device(device)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fed1da-bcaf-4352-8a79-c66b67e93b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #restore the original mask \n",
    "# restored_mask = torch.argmax(one_hot_mask, dim=0)\n",
    "# mask = mask.to(restored_mask.dtype)\n",
    "\n",
    "# # Verify if the restored mask is exactly equal to the original mask\n",
    "# is_equal = torch.allclose(mask, restored_mask)\n",
    "\n",
    "# # Print the result\n",
    "# if is_equal:\n",
    "#     print(\"The restored mask is exactly equal to the original mask.\")\n",
    "# else:\n",
    "#     print(\"The restored mask is not exactly equal to the original mask.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5da28c-9449-4360-b620-8599d8f3c609",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the label colors\n",
    "label_colors = [\n",
    "    [0, 0, 0],       # Background (black)\n",
    "    [255, 0, 0],     # Class 1 (red)\n",
    "    [0, 255, 0],     # Class 2 (green)\n",
    "    [0, 0, 255],     # Class 3 (blue)\n",
    "    [255, 255, 0],   # Class 4 (yellow)\n",
    "    [255, 0, 255],   # Class 5 (magenta)\n",
    "    [0, 255, 255]    # Class 6 (cyan)\n",
    "]\n",
    "\n",
    "# Iterate over the dataset\n",
    "count = 0 \n",
    "for image, _, mask, _ in ds_val:\n",
    "    count = count + 1\n",
    "    if count == 150:\n",
    "        break\n",
    "    old_model.eval()\n",
    "    with torch.no_grad():\n",
    "        result = old_model(image.unsqueeze(0).float())\n",
    "        predictions = torch.argmax(result, dim=1)\n",
    "        pred_labels = predictions.squeeze().cpu().numpy()\n",
    "\n",
    "    # Create a color mask for the ground truth mask by mapping the label values to colors\n",
    "    gt_color_mask = np.zeros((*mask.shape, 3), dtype=np.uint8)\n",
    "    for label in range(7):  # Iterate over labels 0 to 6\n",
    "        gt_color_mask[mask == label] = label_colors[label]\n",
    "\n",
    "    # Create a color mask for the predicted mask by mapping the label values to colors\n",
    "    pred_color_mask = np.zeros((*pred_labels.shape, 3), dtype=np.uint8)\n",
    "    for label in range(7):  # Iterate over labels 0 to 6\n",
    "        pred_color_mask[pred_labels == label] = label_colors[label]\n",
    "\n",
    "    # Plot the input image, ground truth mask, and predicted mask\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "    axes[0].imshow(image.permute(1, 2, 0))\n",
    "    axes[0].set_title('Input Image')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(gt_color_mask)\n",
    "    axes[1].set_title('Ground Truth Mask')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    axes[2].imshow(pred_color_mask)\n",
    "    axes[2].set_title('Predicted Mask')\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd65c0e-146c-4e83-be24-a772f3bdf09c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
