{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffb0bd5-80bf-4fef-8334-525637385b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run dataset.ipynb\n",
    "%run models.ipynb\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from operator import add\n",
    "from PIL import Image\n",
    "import segmentation_models_pytorch as smp\n",
    "import os \n",
    "\n",
    "import re\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score, recall_score\n",
    "import import_ipynb\n",
    "import json\n",
    "from pyfiles.efficientunet import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676a8204-d3cb-49ee-9420-6254fe370204",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file\n",
    "with open('dict_indices.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "train_indices = data['train_indices']\n",
    "val_indices = data['val_indices']\n",
    "\n",
    "with open('domainb_indices.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "domainb_indices = data['train']\n",
    "\n",
    "\n",
    "train_path = os.path.join(os.getcwd(),\"train\")\n",
    "mask_path = os.path.join(os.getcwd(),\"train_gt\")\n",
    "train_x = os.listdir('train')\n",
    "train_x.sort(key=extract_numerical_part)\n",
    "train_y = os.listdir('train_gt')\n",
    "train_y.sort(key=extract_numerical_part)\n",
    "\n",
    "val_path = os.path.join(os.getcwd(),\"val\")\n",
    "val_gt = os.path.join(os.getcwd(),\"val_gt\")\n",
    "val_x = os.listdir('val')\n",
    "val_x.sort(key=extract_numerical_part)\n",
    "val_y = os.listdir('val_gt')\n",
    "val_y.sort(key=extract_numerical_part)\n",
    "\n",
    "testB = os.path.join(os.getcwd(),\"imgs\")\n",
    "test_gtB = os.path.join(os.getcwd(),\"Gts\")\n",
    "path_soft_gtB  =  os.path.join(os.getcwd(),\"soft_gtB\")\n",
    "\n",
    "testB_images = os.listdir(testB)\n",
    "testB_images.sort(key=extract_numerical_part)\n",
    "testB_masks = os.listdir(test_gtB)\n",
    "testB_masks.sort(key=extract_numerical_part)\n",
    "\n",
    "\n",
    "\n",
    "# vss = dess400(train_path,mask_path,train_indices,train_transform)\n",
    "# vall = dess400(val_path,val_gt,val_indices,valid_transform)\n",
    "# dbb = dess400(testB,test_gtB,domainb_indices,train_transform)\n",
    "\n",
    "vs= Dess2(train_path,mask_path,train_x ,train_y , train_transform)\n",
    "\n",
    "val= Dess2(val_path , val_gt, val_x,val_y , valid_transform)\n",
    "domainb = hotencoded(testB , test_gtB , testB_images    , testB_masks , num_classes=7,transform=valid_transform)\n",
    "# domainb_soft = hot400(testB , path_soft_gtB, domainb_indices, num_classes=7,transform=train_transform)\n",
    "domainb_small = hot400(testB , test_gtB,domainb_indices,num_classes=7,transform=valid_transform)\n",
    "ds = hot400(train_path , mask_path,train_indices,num_classes=7,transform=train_transform)\n",
    "ds_val = hot400(val_path , val_gt , val_indices,num_classes=7,transform=valid_transform)\n",
    "# ds = hotencoded(train_path,mask_path , train_x,train_y , num_classes=7,transform=train_transform)\n",
    "# ds_val = hotencoded(val_path,val_gt,val_x,val_y,num_classes=7,transform=valid_transform)\n",
    "dbtrain,dbtest = splitset(testB_masks,0.8)\n",
    "dbtrain_l,dbtrain_ul = splitset(dbtrain,0.10)\n",
    "domainb_train_l = hot400(testB , test_gtB,dbtrain_l,num_classes=7,transform=train_transform)\n",
    "domainb_train_ul = hot400(testB , test_gtB,dbtrain_ul,num_classes=7,transform=train_transform)\n",
    "domainb_test = hot400(testB,test_gtB , dbtest,num_classes=7,transform=valid_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c366c37-faa0-43dc-aa1a-90878c6c2e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.load(\"/home/khan/Desktop/ammar/EfficientUnet-PyTorch-master/ftmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366f0f8f-4d0c-41be-bd96-30998616df56",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0249f5f-b18c-45c1-acd2-1869c90c7852",
   "metadata": {},
   "outputs": [],
   "source": [
    "domainb_test[0][2].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c41333f-6114-4ec7-8c60-f4a92a2e5bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(domainb_test),len(domainb_train_l),len(domainb_train_ul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7c7ec1-3bf6-4416-b42b-92c5be81dde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adccfd6-7c75-4d14-bbee-04952c39293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll=[]\n",
    "for x in domainb_train_l:\n",
    "    _,_,mask,_=x\n",
    "    ll.append(np.unique(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aee322a-96ba-4ab5-9161-ac7d985e7d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for x in ll:\n",
    "    if len(x) >1:\n",
    "        count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580ada11-0551-4a52-901d-4e9d5966d4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "count/len(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20919f1b-21b2-4462-bf01-0e92f4f9e227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function for knowledge distillation\n",
    "def distillation_loss(outputs, labels, teacher_outputs, temperature):\n",
    "    soft_logits = nn.functional.log_softmax(outputs / temperature, dim=1)\n",
    "    soft_targets = nn.functional.softmax(teacher_outputs / temperature, dim=1)\n",
    "    return nn.functional.kl_div(soft_logits, soft_targets, reduction='batchmean')\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c580b164-d701-4b4b-904f-7bb76c63e94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dbtrain_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891c1964-f1dd-440e-abc5-a4248596f7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "teacher_model = smp.Unet(\n",
    "    encoder_name=\"efficientnet-b4\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=7,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "teacher_model.load_state_dict(torch.load(\"efb4-full.pth\" , map_location=torch.device(device)))\n",
    "# teacher_model = get_efficientunet_b4(out_channels=7, concat_input=True, pretrained=True)\n",
    "# teacher_model.load_state_dict(torch.load(\"efb4-full.pth\",map_location = torch.device(device)))\n",
    "# student_model = get_efficientunet_b4(out_channels=7, concat_input=True, pretrained=True)\n",
    "# student_model.load_state_dict(torch.load(\"efb4-full.pth\",map_location = torch.device(device)))\n",
    "student_model = smp.Unet(\n",
    "    \n",
    "    encoder_name=\"efficientnet-b4\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=7,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "student_model.load_state_dict(torch.load(\"efb4-full.pth\",map_location = torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6522e1e6-7f5b-41ce-a39f-1a03c433754b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_mse_loss(input_logits, target_logits):\n",
    "    \"\"\"Takes softmax on both sides and returns MSE loss\n",
    "\n",
    "    Note:\n",
    "    - Returns the sum over all examples. Divide by the batch size afterwards\n",
    "      if you want the mean.\n",
    "    - Sends gradients to inputs but not the targets.\n",
    "    \"\"\"\n",
    "    assert input_logits.size() == target_logits.size(), \"INPUT_LOGITS TARGET_LOGITS MISMATCH\"\n",
    "    input_softmax = F.softmax(input_logits, dim=1)\n",
    "    target_softmax = F.softmax(target_logits, dim=1)\n",
    "    #logging.info(\"input_softmax\"+str(input_softmax.shape))\n",
    "    #logging.info(\"target_softmax\"+str(target_softmax.shape))\n",
    "    mse_loss = (input_softmax - target_softmax) ** 2\n",
    "    return mse_loss\n",
    "\n",
    "def sigmoid_rampup(current, rampup_length):\n",
    "    \"\"\"Exponential rampup from https://arxiv.org/abs/1610.02242\"\"\"\n",
    "    if rampup_length == 0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        current = np.clip(current, 0.0, rampup_length)\n",
    "        phase = 1.0 - current / rampup_length\n",
    "        return float(np.exp(-5.0 * phase * phase))\n",
    "\n",
    "def get_current_consistency_weight(epoch):\n",
    "    # Consistency ramp-up from https://arxiv.org/abs/1610.02242\n",
    "    consistency = 0.1\n",
    "    consistency_rampup = 40.0\n",
    "    return consistency * sigmoid_rampup(epoch, consistency_rampup)\n",
    "    \n",
    "def update_ema_variables(model, ema_model, alpha, global_step):\n",
    "    # Use the true average until the exponential average is more correct\n",
    "    alpha = min(1 - 1 / (global_step + 1), alpha)\n",
    "    for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
    "        ema_param.data.mul_(alpha).add_(1 - alpha, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38df9d55-a87b-4285-9ad7-bd7860694e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d84b10-4984-4518-8c5e-40a6ef205528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # without SEN --ORIGNAL\n",
    "    \n",
    "# def train_mean2d(model, ema_model ,labelled_loader,unlabelled_loader, focal_loss ,optimizer ,iter_num , scheduler, device):\n",
    "#     epoch_loss = 0\n",
    "#     count = 0\n",
    "\n",
    "#     model.train()\n",
    "    \n",
    "#     for i, data in enumerate(zip(labelled_loader ,unlabelled_loader) ):\n",
    "\n",
    "        \n",
    "#         data1, data2 = data\n",
    "#         images, masks,_,_ = data1\n",
    "#         images, masks= images.to(device), masks.to(device)\n",
    "#         outputs = model(images.float())\n",
    "#         outputs_soft = torch.softmax(outputs, dim=1)\n",
    "\n",
    "        \n",
    "\n",
    "#         unlabeled_images = data2[0]\n",
    "#         unlabeled_images = unlabeled_images.to(device)\n",
    "#         unlabeled_outputs = model(unlabeled_images.float())\n",
    "        \n",
    "#         noise = torch.clamp(torch.randn_like(unlabeled_images.float()) *  0.1, -0.2, 0.2)\n",
    "#         ema_inputs = unlabeled_images + noise\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#                 ema_output = ema_model(ema_inputs.float())\n",
    "#                 ema_output_soft = torch.softmax(ema_output, dim=1)\n",
    "        \n",
    "#         # loss_ce = ce_loss(outputs, masks.float())\n",
    "#         # loss_dice = dice_loss(outputs_soft, masks.unsqueeze(1))\n",
    "#         supervised_loss = focal_loss(outputs,masks)\n",
    "#         consistency_weight = get_current_consistency_weight(iter_num // consistency_rampup)\n",
    "#         if iter_num < 400:\n",
    "#             consistency_loss = 0.0\n",
    "#         else:\n",
    "#             consistency_loss = torch.mean((outputs_soft - ema_output_soft) ** 2)\n",
    "#         loss = supervised_loss + consistency_weight * consistency_loss\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         # scheduler.step()\n",
    "        \n",
    "#         update_ema_variables(model, ema_model, ema_decay, iter_num)\n",
    "        \n",
    "#         iter_num = iter_num + 1\n",
    "\n",
    "#         epoch_loss += loss.item() \n",
    "    \n",
    "#     epoch_loss = epoch_loss/len(train_loader)\n",
    "#     return epoch_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596b94f3-e50d-4523-9c90-17b672a03af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mean2d(model, ema_model, labelled_loader, unlabelled_loader, focal_loss, optimizer, iter_num, scheduler, device):\n",
    "    epoch_loss = 0\n",
    "    count = 0\n",
    "    prev_supernoise = None\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for i, data in enumerate(zip(labelled_loader, unlabelled_loader)):\n",
    "\n",
    "        data1, data2 = data\n",
    "        images, masks, _, _ = data1\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        if prev_supernoise is not None:\n",
    "            s_noise = torch.mean(prev_supernoise, dim=1, keepdim=True)\n",
    "            target_size = images.shape[2:]\n",
    "            s_noise = F.interpolate(s_noise, size=target_size, mode='nearest')\n",
    "            s_noise = s_noise.expand(-1, 3, -1, -1)\n",
    "            outputs, _ = model(images.float() + s_noise)\n",
    "        else:\n",
    "            outputs, _ = model(images.float())\n",
    "\n",
    "        outputs_soft = torch.softmax(outputs, dim=1)\n",
    "\n",
    "        unlabeled_images = data2[0]\n",
    "        unlabeled_images = unlabeled_images.to(device)\n",
    "\n",
    "        unlabeled_outputs, _, bases1 = model(unlabeled_images.float())\n",
    "\n",
    "        noise = torch.clamp(torch.randn_like(unlabeled_images.float()) * 0.1, -0.2, 0.2)\n",
    "        ema_inputs = unlabeled_images + noise\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ema_output, supernoise, bases2 = ema_model(ema_inputs.float())\n",
    "            ema_output_soft = torch.softmax(ema_output, dim=1)\n",
    "\n",
    "        # Update the supernoise for the next iteration\n",
    "        prev_supernoise = supernoise\n",
    "\n",
    "        supervised_loss = focal_loss(outputs, masks)\n",
    "\n",
    "        consistency_weight = get_current_consistency_weight(iter_num // consistency_rampup)\n",
    "        if iter_num < 1200:\n",
    "            consistency_loss = 0.0\n",
    "        else:\n",
    "            consistency_loss = torch.mean((outputs_soft - ema_output_soft) ** 2)\n",
    "\n",
    "        # Add low-rank subspace loss\n",
    "        J_s = bases1.transpose(2, 1)  # Transpose bases1 to match the shape of J_t\n",
    "        J_t = bases2\n",
    "        lrs_loss = (2 * J_s.shape[0] - 2 * torch.norm(torch.matmul(J_s.transpose(1, 2), J_t), 'fro')**2) / J_s.shape[0]\n",
    "\n",
    "        loss = supervised_loss + consistency_weight * consistency_loss + lrs_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        update_ema_variables(model, ema_model, ema_decay, iter_num)\n",
    "\n",
    "        iter_num = iter_num + 1\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss = epoch_loss / len(train_loader)\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b11ef69-f26f-42e0-9a86-e0722cfbb706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and criterion\n",
    "\n",
    "checkpoint_path = \".\"\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=0.001)  #1-^3(best) \n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.9)\n",
    "loss_fn = CategoricalFocalLoss(gamma=2.0, alpha=None, reduction='mean')\n",
    "consistency_criterion = softmax_mse_loss\n",
    "# criterion  = CategoricalFocalLoss(gamma=2.0, alpha=None, reduction='mean')\n",
    "\n",
    "# Transfer models to device\n",
    "student_model.to(device)\n",
    "teacher_model.to(device)\n",
    "# old_teacher_model.load_state_dict(torch.load(\"efb4-full.pth\",map_location=torch.device(device)))\n",
    "# old_teacher_model.to(device)\n",
    "\n",
    "\n",
    "num_epochs = 100\n",
    "# Set teacher model\n",
    "import itertools\n",
    "\n",
    "train_loader = DataLoader(ds , batch_size = 8,shuffle=True , num_workers=4)\n",
    "\n",
    "domainb_l_loader = DataLoader(domainb_train_l , batch_size=4 , shuffle=True,num_workers=4)\n",
    "domainb_ul_loader = DataLoader(domainb_train_ul , batch_size=4 , shuffle=True,num_workers=4)\n",
    "\n",
    "# cycle_loader = itertools.cycle(domainb_loader)\n",
    "\n",
    "\n",
    "num_classes=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3932a7f-b27a-4e4b-9032-452197537db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prev_supernoise=None\n",
    "# for i, data in enumerate(zip(domainb_l_loader ,domainb_ul_loader) ):\n",
    "\n",
    "    \n",
    "#     data1, data2 = data\n",
    "#     images, masks,_,_ = data1\n",
    "#     images, masks= images.to(device), masks.to(device)\n",
    "#     outputs,_ = student_model(images.float())\n",
    "#     outputs_soft = torch.softmax(outputs, dim=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     unlabeled_images = data2[0]\n",
    "#     unlabeled_images = unlabeled_images.to(device)\n",
    "#     unlabeled_outputs ,_= student_model(unlabeled_images.float())\n",
    "#     if prev_supernoise is not None:\n",
    "#         noise = torch.mean(prev_supernoise, dim=1, keepdim=True)\n",
    "#         target_size = unlabeled_images.shape[2:]\n",
    "#         noise = F.interpolate(noise, size=target_size, mode='nearest')\n",
    "#         noise=noise.expand(-1,3,-1,-1)\n",
    "    \n",
    "#     else:\n",
    "#         noise = torch.clamp(torch.randn_like(unlabeled_images.float()) * 0.1, -0.2, 0.2)\n",
    "        \n",
    "\n",
    "#     ema_inputs = unlabeled_images + noise\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#                 ema_output ,supernoise= teacher_model(ema_inputs.float())\n",
    "#                 ema_output_soft = torch.softmax(ema_output, dim=1)\n",
    "        \n",
    "#         # Update the supernoise for the next iteration\n",
    "#     prev_supernoise = supernoise\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96a2631-def7-4d64-838f-041f334ee982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_dice = DiceLoss(n_classes=7,softmax=True,weight=[1,2,1,2,2,2,1])\n",
    "max_iterations = 400 #close to few shot method \n",
    "iter_num = 0\n",
    "ema_decay = 0.99\n",
    "\n",
    "consistency_rampup = 200.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dec0b7-1cb0-4d07-b56a-9b4d1dac801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_student_loss = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266720aa-f1bc-41e9-bb9a-79c21d130f6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epoch_data = {}  # Create an empty dictionary to store epoch data\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train_mean2d(student_model, teacher_model, domainb_l_loader, domainb_ul_loader, loss_fn, optimizer, iter_num, scheduler,device)\n",
    "    t_loss, t_f1, t_f1_class = validlossf2(teacher_model, domainb_test, loss_fn, num_classes=7)\n",
    "    s_loss, s_f1, s_f1_class = validlossf2(student_model, domainb_test, loss_fn, num_classes=7)\n",
    "\n",
    "    # Print the losses and F1 scores\n",
    "    print(f\"Epoch: {epoch+1:02} | Train Loss: {train_loss:.6f} | Teacher Loss: {t_loss:.6f} | Student Loss: {s_loss:.6f}\")\n",
    "    print(f\"Teacher F1: {t_f1:.3f} | Student F1: {s_f1:.3f}\")\n",
    "\n",
    "\n",
    "    # if best_student_loss > s_loss:\n",
    "    #     print(f\"Student f1 improved from {best_student_loss:.6f} to {s_loss:.6f}. Saving checkpoint: {checkpoint_path}\")\n",
    "    #     best_student_loss = s_loss\n",
    "    #     torch.save(student_model.state_dict(), \"partitions/zero/5000.pth\") #partitions/twentyfivr \n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "    data_str = f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins:.0f}m {epoch_secs:.0f}s\\n'\n",
    "    data_str += f'\\tTrain Loss: {train_loss:.6f} | Teacher Loss: {t_loss:.6f} | Student Loss: {s_loss:.6f}\\n'\n",
    "    data_str += f'\\tTeacher F1: {t_f1:.3f} | Student F1: {s_f1:.3f}\\n'\n",
    "     # Append epoch data to the dictionary\n",
    "    epoch_data[epoch+1] = {\n",
    "        'train_loss': train_loss,\n",
    "        'teacher_loss': t_loss,\n",
    "        'student_loss': s_loss,\n",
    "        'teacher_f1': t_f1,\n",
    "        'student_f1': s_f1\n",
    "    }\n",
    "    # Print the epoch summary\n",
    "    print(data_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdf6bf1-6804-4be7-a584-99e9330d2322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa21307-74fc-4472-916c-d1ce2f33e728",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(epoch_data, orient='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecba3517-8c2c-4d34-9b44-ce7df05386b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"epoch_data_0.10.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0548abb-8282-4047-911f-73270ca24b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple kd epoch 20 #lr0.001, epoch 9=0.777, epoch20=0.815 (many oscillations)---pefect config since lr change doesnt work \n",
    "#for above try initializing both simple networks of teachers weight \n",
    "#excel record asap, scehduler step, record the entire steps in excel \n",
    "# svd(student&teacher) 30 samelr,epoch=24 0.815  but all oscillatory --30 more epochs --0.826 at epoch 8 \n",
    "#simple same weights(student and teacher ) 0.830 at 37 ,0.836 at 56\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedf6f84-ecdf-4340-86b0-b03bbed4fb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model names \n",
    "#studentsvdSW_efb4_257520.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75a22c0-cc68-46ac-918f-9db599a8fe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model.load_state_dict(torch.load( \"studentsvdSW2_efb4_257520.pth\"))\n",
    "# old_teacher_model.load_state_dict(torch.load(\"efb4-full.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85926441-57b0-4b92-ab43-453f5cd24d67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "ine,t,tt= validlossf2(student_model,domainb_test,loss_fn,num_classes)\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad383be-a87c-4dbd-b532-276505c3cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67378fc3-3d4a-4c69-b9da-c342dd9f39d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=0\n",
    "for x in tt :\n",
    "    if x>0.9:\n",
    "        pass\n",
    "    else:\n",
    "        y=y+x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f5ff34-18f2-424f-b2a2-8087459833e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "l,c,a,d = validlossf3(student_model,domainb_test,loss_fn,num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5ace25-e936-4ede-9bac-280d64a0af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442c9b62-aa5e-495b-ab70-1cdb7992309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# Assuming `d` is your DataFrame\n",
    "d = d.iloc[:330, 1:]  # Ignore the first field and limit to first 330 rows\n",
    "\n",
    "# Replace NaN values with 0\n",
    "d = d.fillna(0)\n",
    "\n",
    "# Create individual line plots for each field\n",
    "for column in d.columns:\n",
    "    plt.figure()  # Create a new figure for each plot\n",
    "    plt.plot(d.index, d[column])\n",
    "    plt.title(f\"Line Plot for {column}\")\n",
    "    plt.xlabel(\"Row Number\")\n",
    "    plt.ylabel(\"Field Value\")\n",
    "\n",
    "# Show all the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6cd6b6-f4c8-4d97-b3b0-b11d4cc28f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openpyxl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c35f724-84bc-4ed6-8495-e55d69d323c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.to_excel(\"df.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8af677e-9ac1-4b4e-86b5-782278529559",
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e16d3be-ed6f-4d82-9b8b-767610c32765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validlossf3(model, ds, loss_fn, num_classes):\n",
    "    f1_list = []\n",
    "    loss_list = []\n",
    "    \n",
    "    model.eval()\n",
    "    for img, mask, orig, _ in ds:\n",
    "        img = img.to(device)\n",
    "        mask = mask.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(img.float().unsqueeze(0))\n",
    "        loss = loss_fn(output, mask.float().unsqueeze(0))\n",
    "        pred = torch.argmax(output, dim=1).squeeze(0)\n",
    "        pred = pred.to(\"cpu\")\n",
    "        \n",
    "        f1_scores = np.zeros(num_classes)\n",
    "        \n",
    "        for class_label in range(num_classes):  # F1 score and loss for the current class\n",
    "            pred_class = (pred == class_label)\n",
    "            real_class = (orig == class_label)\n",
    "            \n",
    "            true_positives = np.sum(np.logical_and(pred_class, real_class).numpy())\n",
    "            false_positives = np.sum(np.logical_and(pred_class, ~real_class).numpy())\n",
    "            false_negatives = np.sum(np.logical_and(~pred_class, real_class).numpy())\n",
    "            \n",
    "            precision = true_positives / (true_positives + false_positives)\n",
    "            recall = true_positives / (true_positives + false_negatives)\n",
    "            \n",
    "            f1_scores[class_label] = 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "        f1_list.append(np.round(list(f1_scores),5))\n",
    "        loss_list.append(loss.item())\n",
    "      \n",
    "    average_loss = np.average(loss_list)\n",
    "    f1df = pd.DataFrame(f1_list)\n",
    "    f1avg = f1df[f1df != 0].mean()\n",
    "    average_f1 = np.round(np.average(f1avg),5)\n",
    "\n",
    "    return np.round(average_loss,7), np.round(average_f1,5), np.round(f1avg,5),f1df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18775be5-39a9-4c41-8b29-a9eac12bf8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ine,t,tt= validlossf1(teacher_model,domainb_test,loss_fn,num_classes)\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4be8c7b-3eef-43bb-b4e6-62b58d9393f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "i,t,tt=validlossf2(old_teacher_model,domainb_test,loss_fn,num_classes)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1239999c-d545-496e-866e-e4039d5d3e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model1 = teacher_model1.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a41e196-474e-4c8f-95f0-c0d7f1ac8a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(domainb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cde6db9-f3c0-436f-a6b2-824faa0910b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model1 = smp.Unet(\n",
    "    encoder_name=\"efficientnet-b4\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=7,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "teacher_model1.load_state_dict(torch.load(\"efb4-full.pth\" , map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633611aa-413f-435d-8c83-f2f831318298",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model.load_state_dict(torch.load(\"newefb4svd-full.pth\",map_location = torch.device(device)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df11dfd1-ad29-405f-8f1c-17c94cc69a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "i,t,tt=validlossf1(teacher_model,domainb_test,loss_fn,num_classes=7)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee790bc-71dc-4b40-ab02-951bf67fdd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "i,t,tt=validlossf1(teacher_model1,domainb_train_ul,loss_fn,num_classes=7)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d47c58-e5e3-4992-8cfa-4cbe2164a0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "i,t,tt=validlossf1(teacher_model1,domainb_train_l,loss_fn,num_classes=7)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37e0e9d-5a2a-4810-9468-63a8b314efc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model.load_state_dict(torch.load(\"tea\"))\n",
    "i,t,tt=validlossf1(teacher_model,domainb_test,loss_fn,num_classes=7)\n",
    "t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
