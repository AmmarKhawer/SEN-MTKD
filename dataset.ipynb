{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54162f11-9350-45d0-80a5-d147b0e5f042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe4e49ee-1f5a-4a56-b635-f43055ffab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dess2(Dataset):\n",
    "    def __init__(self, train_path,mask_path , images_list, masks_list, transform=None):\n",
    "        self.images_list = images_list\n",
    "        self.masks_list = masks_list\n",
    "        self.train_path = train_path\n",
    "        self.mask_path = mask_path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = cv2.imread(os.path.join(self.train_path, self.images_list[index]))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "\n",
    "        mask = cv2.imread(os.path.join(self.mask_path, self.masks_list[index]), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Apply transformation if specified\n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed['image']\n",
    "            mask = transformed['mask']\n",
    "\n",
    "        return image, mask,  self.images_list[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_list)\n",
    "        \n",
    "\n",
    "class dess400(Dataset):\n",
    "    def __init__(self, train_path,mask_path , indices, transform=None):\n",
    "        self.train_path = train_path\n",
    "        self.mask_path = mask_path\n",
    "        self.n_samples = len(indices)\n",
    "        self.transform = transform\n",
    "        self.indices = indices\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        name = self.indices[index]\n",
    "\n",
    "        image = cv2.imread(os.path.join(self.train_path, name))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "\n",
    "        mask = cv2.imread(os.path.join(self.mask_path, name), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Apply transformation if specified\n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed['image']\n",
    "            mask = transformed['mask']\n",
    "\n",
    "        return image, mask , name\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bf18d24-40de-4475-a2a5-f5e0a5694fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import re\n",
    "\n",
    "\n",
    "def extract_numerical_part(element):\n",
    "    match = re.search(r'\\d+', element)\n",
    "    if match:\n",
    "        return int(match.group())\n",
    "    else:\n",
    "        return 0\n",
    "def round_clip_0_1(x, **kwargs):\n",
    "    return x.round().clip(0, 1)\n",
    "\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n",
    "    A.PadIfNeeded(min_height=384, min_width=384, always_apply=True, border_mode=cv2.BORDER_REPLICATE),\n",
    "    #A.RandomCrop(height=320, width=320, always_apply=True),\n",
    "    A.GaussNoise(p=0.2),\n",
    "    A.Perspective(p=0.5),\n",
    "    A.OneOf(\n",
    "        [\n",
    "            A.Sharpen(p=1),\n",
    "            A.Blur(blur_limit=3, p=1),\n",
    "            A.MotionBlur(blur_limit=3, p=1),\n",
    "        ],\n",
    "        p=0.9,\n",
    "    ),\n",
    "    \n",
    "    ToTensorV2(),  # Convert image and mask to tensors\n",
    "])\n",
    "    #A.Lambda(mask=round_clip_0_1),\n",
    "\n",
    "valid_transform = A.Compose([\n",
    "    A.Resize(384, 384),\n",
    "     # Add normalization if required\n",
    "    ToTensorV2()  # Convert image to tensor\n",
    "])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "905fa5ad-6ffb-4946-bde7-3bdbb9eb5d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#________________________breaking datasets__________________________________make finction asap!!!!\n",
    "\n",
    "# train_indices=[]\n",
    "# for x in range(3200):\n",
    "#         train_indices.append(vs[x][2])\n",
    "# val_indices=[]\n",
    "\n",
    "# for x in range(1200):\n",
    "#         val_indices.append(val[x][2])\n",
    "\n",
    "#_____save dict indices___ \n",
    "# dict_indices = {\"train_indices\": train_indices, \"val_indices\": val_indices}\n",
    "# #save the file \n",
    "# with open('dict_indices.json', 'w') as file:\n",
    "#     json.dump(dict_indices, file)\n",
    "\n",
    "##domainb_train indices \n",
    "# domainb_train=[]\n",
    "# for x in range(1200):\n",
    "#         domainb_train.append(domainb_all[x][2])\n",
    "\n",
    "# domainb_indices = {\"train\": domainb_train}\n",
    "# #save the file \n",
    "# with open('domainb_indices.json', 'w') as file:\n",
    "#     json.dump(domainb_indices, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab9e6d80-65c9-47df-ae5a-2e6bc5fae9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #loading the json file\n",
    "# import json\n",
    "# # Load the JSON file\n",
    "# with open('dict_indices.json', 'r') as file:\n",
    "#     data = json.load(file)\n",
    "# train_indices = data['train_indices']\n",
    "# val_indices = data['val_indices']\n",
    "\n",
    "# # Load the JSON file\n",
    "# with open('domainb_indices.json', 'r') as file:\n",
    "#     data = json.load(file)\n",
    "\n",
    "# domainb_train = data['train']\n",
    "# domainb_val = data['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a76d48f-3ca0-462c-be93-2c1d75b8672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n",
    "#\n",
    "# This work is licensed under the Creative Commons Attribution-NonCommercial\n",
    "# 4.0 International License. To view a copy of this license, visit\n",
    "# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n",
    "# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n",
    "\n",
    "\"\"\"Functions for ramping hyperparameters up or down\n",
    "\n",
    "Each function takes the current training step or epoch, and the\n",
    "ramp length in the same format, and returns a multiplier between\n",
    "0 and 1.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid_rampup(current, rampup_length):\n",
    "    \"\"\"Exponential rampup from https://arxiv.org/abs/1610.02242\"\"\"\n",
    "    if rampup_length == 0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        current = np.clip(current, 0.0, rampup_length)\n",
    "        phase = 1.0 - current / rampup_length\n",
    "        return float(np.exp(-5.0 * phase * phase))\n",
    "\n",
    "\n",
    "def linear_rampup(current, rampup_length):\n",
    "    \"\"\"Linear rampup\"\"\"\n",
    "    assert current >= 0 and rampup_length >= 0\n",
    "    if current >= rampup_length:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return current / rampup_length\n",
    "\n",
    "\n",
    "def cosine_rampdown(current, rampdown_length):\n",
    "    \"\"\"Cosine rampdown from https://arxiv.org/abs/1608.03983\"\"\"\n",
    "    assert 0 <= current <= rampdown_length\n",
    "    return float(.5 * (np.cos(np.pi * current / rampdown_length) + 1))\n",
    "\n",
    "def get_current_consistency_weight(epoch):\n",
    "    # Consistency ramp-up from https://arxiv.org/abs/1610.02242\n",
    "    return consistency * sigmoid_rampup(epoch, consistency_rampup)\n",
    "\n",
    "\n",
    "def update_ema_variables(model, ema_model, alpha, global_step):\n",
    "    # Use the true average until the exponential average is more correct\n",
    "    alpha = min(1 - 1 / (global_step + 1), alpha)\n",
    "    for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
    "        ema_param.data.mul_(float(alpha)).add_(1 - float(alpha), param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a7294df-ef58-4c77-9fcf-a7fdae14b6d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9add7449-493e-477d-882a-4f4a5954b02f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "175a50f6-5b0f-4cc2-908b-49ef0b5ccaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e0e20f6-c298-4e2e-b874-c1c773180b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mix(Dataset):\n",
    "    def __init__(self, train_path, mask_path, testB, test_gtB, mix_images, transform=None):\n",
    "        self.mix_images = mix_images\n",
    "        self.train_path = train_path\n",
    "        self.mask_path = mask_path\n",
    "        self.b_path = testB\n",
    "        self.b_mask_path = test_gtB\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((384, 384)),\n",
    "\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.4814, 0.4494, 0.3958],\n",
    "                                 std=[0.2563, 0.2516, 0.2601])]\n",
    "        )\n",
    "        self.noise_transform = transforms.Compose([\n",
    "\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.2, 0.2), scale=(0.8, 1.2), shear=10),\n",
    "            transforms.GaussianBlur(3, sigma=(0.1, 2.0))]\n",
    "        )\n",
    "        self.mask_transform = transforms.Compose([\n",
    "            transforms.Resize(384),\n",
    "\n",
    "            transforms.PILToTensor()\n",
    "        ])\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        full_path = self.mix_images[index]\n",
    "        image = Image.open(full_path).convert(\"RGB\")\n",
    "        \n",
    "    \n",
    "        image_ema = self.noise_transform(image)\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        if full_path.split(\"/\")[-2] == \"train\":\n",
    "            mask = Image.open(os.path.join(self.mask_path, full_path.split(\"/\")[-1])).convert(\"L\")\n",
    "            mask = self.mask_transform(mask)\n",
    "            return image, image_ema ,mask,mask\n",
    "        else:\n",
    "            mask = Image.open(os.path.join(self.b_mask_path, full_path.split(\"/\")[-1])).convert(\"L\")\n",
    "            mask = self.mask_transform(mask)\n",
    "            empty_mask = torch.full((1,384,384), -1, dtype=torch.int64)\n",
    "            \n",
    "            return image, image,empty_mask , mask\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mix_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "feda3312-e93b-4ca2-9ca6-83bb4465121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "\n",
    "        A.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n",
    "\n",
    "        A.PadIfNeeded(min_height=384, min_width=384, always_apply=True, border_mode=cv2.BORDER_REPLICATE),\n",
    "        #A.RandomCrop(height=320, width=320, always_apply=True),\n",
    "\n",
    "        A.IAAAdditiveGaussianNoise(p=0.2),\n",
    "        A.IAAPerspective(p=0.5),\n",
    "\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.IAASharpen(p=1),\n",
    "                A.Blur(blur_limit=3, p=1),\n",
    "                A.MotionBlur(blur_limit=3, p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        A.Lambda(mask=round_clip_0_1)\n",
    "    ]\n",
    "    return A.Compose(train_transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76d08fad-beef-4ae2-89ef-daa2f57724ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class hotencoded(Dataset):\n",
    "    def __init__(self, train_path, mask_path, images_list, masks_list, num_classes,transform=None):\n",
    "        self.images_list = images_list\n",
    "        self.masks_list = masks_list\n",
    "        self.train_path = train_path\n",
    "        self.mask_path = mask_path\n",
    "        self.transform = transform\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = cv2.imread(os.path.join(self.train_path, self.images_list[index]))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "\n",
    "        mask = cv2.imread(os.path.join(self.mask_path, self.masks_list[index]), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Apply transformation if specified\n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed['image']\n",
    "            mask = transformed['mask']\n",
    "\n",
    "        # Perform one-hot encoding on the mask\n",
    "        one_hot_mask = torch.zeros((self.num_classes, *mask.shape), dtype=torch.float32)\n",
    "\n",
    "        # Expand dimensions of `mask` to match the shape of `one_hot_mask`\n",
    "        expanded_mask = mask.unsqueeze(0)\n",
    "\n",
    "        # Convert `expanded_mask` to `torch.int64`\n",
    "        expanded_mask = expanded_mask.to(torch.int64)\n",
    "\n",
    "        # Use `torch.scatter_` to perform the one-hot encoding\n",
    "        one_hot_mask = one_hot_mask.scatter_(0, expanded_mask, 1)\n",
    "\n",
    "        return image, one_hot_mask, mask, self.images_list[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_list)\n",
    "\n",
    "\n",
    "\n",
    "class hot400(Dataset):\n",
    "    def __init__(self, train_path,mask_path , indices, num_classes,transform=None):\n",
    "        self.train_path = train_path\n",
    "        self.mask_path = mask_path\n",
    "        self.n_samples = len(indices)\n",
    "        self.transform = transform\n",
    "        self.indices = indices\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        name = self.indices[index]\n",
    "\n",
    "        image = cv2.imread(os.path.join(self.train_path, name))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "\n",
    "        mask = cv2.imread(os.path.join(self.mask_path, name), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Apply transformation if specified\n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed['image']\n",
    "            mask = transformed['mask']\n",
    "        # Perform one-hot encoding on the mask\n",
    "        one_hot_mask = torch.zeros((self.num_classes, *mask.shape), dtype=torch.float32)\n",
    "\n",
    "        # Expand dimensions of `mask` to match the shape of `one_hot_mask`\n",
    "        expanded_mask = mask.unsqueeze(0)\n",
    "\n",
    "        # Convert `expanded_mask` to `torch.int64`\n",
    "        expanded_mask = expanded_mask.to(torch.int64)\n",
    "\n",
    "        # Use `torch.scatter_` to perform the one-hot encoding\n",
    "        one_hot_mask = one_hot_mask.scatter_(0, expanded_mask, 1)\n",
    "\n",
    "        return image, one_hot_mask, mask, name\n",
    "\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "959093ee-4c24-4e8d-ae20-8d977c2f1f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = hot400(train_path , mask_path,train_indices,num_classes=7,transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "016b3dc7-eaf2-4bd1-965c-322c57f8ffcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Restore the original mask\n",
    "# restored_mask = torch.argmax(mask, dim=0)\n",
    "# orig = orig.to(restored_mask.device)  # Move `orig` tensor to the same device as `restored_mask`\n",
    "\n",
    "# # Verify if the restored mask is exactly equal to the original mask\n",
    "# is_equal = torch.allclose(orig, restored_mask)\n",
    "\n",
    "# # Print the result\n",
    "# if is_equal:\n",
    "#     print(\"The restored mask is exactly equal to the original mask.\")\n",
    "# else:\n",
    "#     print(\"The restored mask is not exactly equal to the original mask.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57daeddf-aed5-4cfb-ab7e-852d69e231c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "\n",
    "# Load the JSON file\n",
    "with open('dict_indices.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "train_indices = data['train_indices']\n",
    "val_indices = data['val_indices']\n",
    "\n",
    "with open('domainb_indices.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "domainb_indices = data['train']\n",
    "\n",
    "\n",
    "train_path = os.path.join(os.getcwd(),\"train\")\n",
    "mask_path = os.path.join(os.getcwd(),\"train_gt\")\n",
    "train_x = os.listdir('train')\n",
    "train_x.sort(key=extract_numerical_part)\n",
    "train_y = os.listdir('train_gt')\n",
    "train_y.sort(key=extract_numerical_part)\n",
    "\n",
    "val_path = os.path.join(os.getcwd(),\"val\")\n",
    "val_gt = os.path.join(os.getcwd(),\"val_gt\")\n",
    "val_x = os.listdir('val')\n",
    "val_x.sort(key=extract_numerical_part)\n",
    "val_y = os.listdir('val_gt')\n",
    "val_y.sort(key=extract_numerical_part)\n",
    "\n",
    "testB = os.path.join(os.getcwd(),\"imgs\")\n",
    "test_gtB = os.path.join(os.getcwd(),\"Gts\")\n",
    "path_soft_gtB  =  os.path.join(os.getcwd(),\"soft_gtB\")\n",
    "\n",
    "testB_images = os.listdir(testB)\n",
    "testB_images.sort(key=extract_numerical_part)\n",
    "testB_masks = os.listdir(test_gtB)\n",
    "testB_masks.sort(key=extract_numerical_part)\n",
    "\n",
    "\n",
    "\n",
    "vss = dess400(train_path,mask_path,train_indices,train_transform)\n",
    "vall = dess400(val_path,val_gt,val_indices,train_transform)\n",
    "dbb = dess400(testB,test_gtB,domainb_indices,train_transform)\n",
    "domainb_soft = dess400(testB , path_soft_gtB, domainb_indices, train_transform)\n",
    "# softb = dess400(testB,path_soft_gtB,domainb_train , train_transform)\n",
    "# testb_val = dess400(testB, test_gtB ,domainb_val,train_transform)\n",
    "\n",
    "# combined_ds_train = ConcatDataset([vss,softb ])\n",
    "\n",
    "\n",
    "# train_loader = DataLoader(vss, batch_size=5, shuffle=True)\n",
    "# val_loader = DataLoader(vall, batch_size=15, shuffle=False)\n",
    "\n",
    "# mix_loader = DataLoader(combined_ds_train, batch_size=10 , shuffle=True)\n",
    "\n",
    "vs= Dess2(train_path,mask_path,train_x ,train_y , train_transform)\n",
    "\n",
    "val= Dess2(val_path , val_gt, val_x,val_y , train_transform)\n",
    "domainb = hotencoded(testB , test_gtB , testB_images    , testB_masks , num_classes=7,transform=valid_transform)\n",
    "# ds = hot400(train_path , mask_path,train_indices,num_classes=7,transform=train_transform)\n",
    "# ds_val = hot400(val_path , val_gt , val_indices,num_classes=7,transform=train_transform)\n",
    "ds = hotencoded(train_path,mask_path , train_x,train_y , num_classes=7,transform=train_transform)\n",
    "ds_val = hotencoded(val_path,val_gt,val_x,val_y,num_classes=7,transform=valid_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "404ff67f-174b-4657-876a-9d2a95a0bc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = \"/home/khan/Desktop/ammar/MT-UDA-main/data/PnpAda_release_data/train&val/mr_train_tfs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61227d47-c500-4d78-a3a3-e23797412fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitset(ids,factor):\n",
    "    split_index = int(len(ids) * factor)\n",
    "    train= ids[:split_index]\n",
    "    test = ids[split_index:]\n",
    "    return train ,test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "92419920-a917-4787-8e45-32dbef6da1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')\n",
    "\n",
    "# class Dataset:\n",
    "#     def __init__(self, root, transform=None):\n",
    "#         self.root = root\n",
    "#         self.imgs = []\n",
    "#         for dir, _, fnames in sorted(os.walk(root, followlinks=True)):\n",
    "#             for fname in sorted(fnames):\n",
    "#                 if fname.lower().endswith(IMG_EXTENSIONS):\n",
    "#                     self.imgs.append(os.path.join(dir, fname))\n",
    "\n",
    "#         self.transform = transform if transform is not None else lambda x: x\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         with open(self.imgs[index], 'rb') as f:\n",
    "#             img = Image.open(f)\n",
    "#             return self.transform(img.convert('RGB'))\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53ac1f89-b166-4335-8da8-7e66b8df9bf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.hotencoded at 0x7f9103d950c0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de80c884-3a40-4d7f-85f9-911440efcc3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
